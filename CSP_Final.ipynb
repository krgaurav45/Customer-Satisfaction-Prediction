{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Satisfaction Prediction - Brazillian e-Commerce Public Dataset\n",
    "## 1. Business Problem:-\n",
    "### 1.1 Description \n",
    "\n",
    "This is a Brazilian ecommerce public dataset of orders made at Olist Store. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers. A geolocation dataset that relates Brazilian zip codes to lat/lng coordinates has also been released.\n",
    "\n",
    "This dataset was generously provided by Olist, the largest department store in Brazilian marketplaces. Olist connects small businesses from all over Brazil to channels without hassle and with a single contract. Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners. See more on the website: www.olist.com\n",
    "\n",
    "After a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some comments.\n",
    "\n",
    "CREDITS:- Kaggle\n",
    "    \n",
    "### 1.2 Problem Statement \n",
    "Predict Customer satisfaction of the purhase from the olist e-commerce site.\n",
    "\n",
    "### 1.3 Sources/Useful Links\n",
    "\n",
    "1. Source:- https://www.kaggle.com/olistbr/brazilian-ecommerce\n",
    "2. Data Description:- https://www.kaggle.com/andresionek/understanding-the-olist-ecommerce-dataset\n",
    "3. Discussion:- https://www.kaggle.com/olistbr/brazilian-ecommerce/discussion/66466\n",
    "4. Data Analysis:- https://www.kaggle.com/duygut/brazilian-e-commerce-data-analysis\n",
    "5. Existing Approach:- https://www.kaggle.com/andresionek/predicting-customer-satisfaction\n",
    "6. Ensemble:- https://pdfs.semanticscholar.org/449e/7116d7e2cff37b4d3b1357a23953231b4709.pdf\n",
    "7. Sentiment:- https://www.kaggle.com/thiagopanini/e-commerce-sentiment-analysis-eda-viz-nlp\n",
    "\n",
    "#### 1.3.1 Real world/Business Objectives and Constraints \n",
    "1. No strict latency concerns.\n",
    "2. Interpretability is important.\n",
    "\n",
    "## 2. Machine Learning Probelm \n",
    "### 2.1 Data \n",
    "#### 2.1.1 Data Overview \n",
    "\n",
    "Source:- https://www.kaggle.com/olistbr/brazilian-ecommerce\n",
    "\n",
    "The data is divided in multiple datasets for better understanding and organization. Please refer to the following data schema when working with it:\n",
    "<img src=\"https://i.imgur.com/HRhd2Y0.png\" />\n",
    "\n",
    "#### 2.1.2 Data Description\n",
    "The **olist_orders_dataset** have the order data for each purchase connected with other data using order_id and customer_id.\n",
    "The **olist_order_reviews_dataset** have the labeled review data for each order in the order data table labelled as [1,2,3,4,5] where 5 being the highest and 1 being the lowest.\n",
    "We will use reviews greater than 3 as positive and less than equal to 3 as negative review.\n",
    "The table will be joined accordingly to get the data needed for the analysis, feature selection and model training.\n",
    "\n",
    "### 2.2 Mapping the real world problem to an ML problem \n",
    "#### 2.2.1 Type of Machine Leaning Problem\n",
    "It is a binary classification problem, for a given purchase order we need to predict if it will get a positive or negative review from the customer.\n",
    "\n",
    "#### 2.2.2 Performance Metric \n",
    "Metric(s): \n",
    "* f1-score : https://www.kaggle.com/wiki/LogarithmicLoss\n",
    "* Binary Confusion Matrix\n",
    "\n",
    "### 2.3 Train and Test Construction\n",
    "\n",
    "We build train and test by stratified random split of the data in the ratio of 70:30 or 80:20 whatever we choose as we have sufficient points to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/krgsharma17/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/krgsharma17/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib\n",
    "matplotlib.use(u'nbAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import log_loss,accuracy_score, confusion_matrix, f1_score\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (94652, 33) (94652,)\n",
      "Train data:  (23663, 33) (23663,)\n"
     ]
    }
   ],
   "source": [
    "# loading the data tables\n",
    "customer_data = pd.read_csv('olist_customers_dataset.csv')\n",
    "geolocation_data = pd.read_csv('olist_geolocation_dataset.csv')\n",
    "order_items_dataset = pd.read_csv('olist_order_items_dataset.csv')\n",
    "order_payments_dataset = pd.read_csv('olist_order_payments_dataset.csv')\n",
    "order_reviews_dataset = pd.read_csv('olist_order_reviews_dataset.csv')\n",
    "order_dataset = pd.read_csv('olist_orders_dataset.csv')\n",
    "order_products_dataset = pd.read_csv('olist_products_dataset.csv')\n",
    "order_sellers_dataset = pd.read_csv('olist_sellers_dataset.csv')\n",
    "product_translation_dataset = pd.read_csv('product_category_name_translation.csv')\n",
    "\n",
    "order_reviews_dataset = order_reviews_dataset[['order_id','review_score', 'review_comment_message']]\n",
    "order_review_data = order_reviews_dataset.merge(order_dataset,on='order_id')\n",
    "order_products_dataset_english = pd.merge(order_products_dataset,product_translation_dataset,on='product_category_name'\n",
    "                                          ,how='left')\n",
    "order_products_dataset_english = order_products_dataset_english.drop(labels='product_category_name',axis=1)\n",
    "order_product_item_dataset = pd.merge(order_items_dataset,order_products_dataset_english,on='product_id')\n",
    "ordered_product_reviews = pd.merge(order_product_item_dataset,order_review_data,on='order_id')\n",
    "ordered_product_reviews_payments = pd.merge(ordered_product_reviews,order_payments_dataset,on='order_id')\n",
    "df_final = pd.merge(ordered_product_reviews_payments,customer_data,on='customer_id')\n",
    "\n",
    "product_id = order_product_item_dataset.groupby('product_id').count()['seller_id'].index\n",
    "seller_count = order_product_item_dataset.groupby('product_id').count()['seller_id'].values\n",
    "product_seller_count = pd.DataFrame({'product_id':product_id,'sellers_count':seller_count})\n",
    "\n",
    "order_id = order_product_item_dataset.groupby('order_id').count()['product_id'].index\n",
    "pd_count = order_product_item_dataset.groupby('order_id').count()['product_id'].values\n",
    "order_items_count = pd.DataFrame({'order_id':order_id,'products_count':pd_count})\n",
    "\n",
    "df_final = pd.merge(df_final,product_seller_count,on='product_id')\n",
    "df_final = pd.merge(df_final,order_items_count,on='order_id')\n",
    "\n",
    "# df = pd.read_csv('olist_final.csv')\n",
    "\n",
    "# separating the target variable\n",
    "y = df_final['review_score']\n",
    "X = df_final.drop(labels='review_score',axis=1)\n",
    "\n",
    "# train test 80:20 split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=25)\n",
    "print(\"Train data: \",X_train.shape,y_train.shape)\n",
    "print(\"Train data: \",X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts(texts):\n",
    "    \n",
    "    processed_text = []\n",
    "    \n",
    "    portuguese_stopwords = stopwords.words('portuguese') # portugese language stopwords\n",
    "    stemmer = RSLPStemmer() # portugese language stemmer\n",
    "    \n",
    "    links = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' # check for hyperlinks\n",
    "    dates = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}' # check for dates\n",
    "    currency = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+' # check for currency symbols\n",
    "    \n",
    "    for text in texts:\n",
    "        text = re.sub('[\\n\\r]', ' ', text) # remove new lines\n",
    "        text = re.sub(links, ' URL ', text) # remove hyperlinks\n",
    "        text = re.sub(dates, ' ', text) # remove dates\n",
    "        text = re.sub(currency, ' dinheiro ', text) # remove currency symbols\n",
    "        text = re.sub('[0-9]+', ' numero ', text) # remove digits\n",
    "        text = re.sub('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', ' negação ', text) # replace no with negative\n",
    "        text = re.sub('\\W', ' ', text) # remove extra whitespaces\n",
    "        text = re.sub('\\s+', ' ', text) # remove extra spaces\n",
    "        text = re.sub('[ \\t]+$', '', text) # remove tabs etc.\n",
    "        text = ' '.join(e for e in text.split() if e.lower() not in portuguese_stopwords) # remove stopwords\n",
    "#         text = ' '.join(stemmer.stem(e.lower()) for e in text.split()) # stemming the words\n",
    "        processed_text.append(text.lower().strip())\n",
    "        \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfWord2Vector(text,glove_words,tfidf_words,tf_values):\n",
    "    w2vmodel = Word2Vec.load(\"word2vec.model\")\n",
    "    # compute average word2vec for each review.\n",
    "    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "    for sentence in tqdm(text): # for each review/sentence\n",
    "        vector = np.zeros(300) # as word vectors are of zero length\n",
    "        tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "        for word in sentence.split(): # for each word in a review/sentence\n",
    "            if (word in glove_words) and (word in tfidf_words):\n",
    "                vec = w2vmodel.wv[word] # embeddings[word] \n",
    "                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "                tf_idf = tf_values[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "                tf_idf_weight += tf_idf\n",
    "        if tf_idf_weight != 0:\n",
    "            vector /= tf_idf_weight\n",
    "        tfidf_w2v_vectors.append(vector)\n",
    "    tfidf_w2v_vectors = np.asarray(tfidf_w2v_vectors)\n",
    "    \n",
    "    return tfidf_w2v_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(X):\n",
    "    \n",
    "    datetime_cols = ['order_purchase_timestamp','order_approved_at','order_delivered_customer_date',\n",
    "                     'order_estimated_delivery_date','order_delivered_carrier_date']\n",
    "    for col in datetime_cols:\n",
    "        X[col] = pd.to_datetime(X[col]).dt.date\n",
    "    # calculating estimated delivery time\n",
    "    X['estimated_delivery_time'] = (X['order_estimated_delivery_date'] - X['order_approved_at']).dt.days\n",
    "\n",
    "    # calculating actual delivery time\n",
    "    X['actual_delivery_time'] = (X['order_delivered_customer_date'] - X['order_approved_at']).dt.days\n",
    "\n",
    "    # calculating diff_in_delivery_time\n",
    "    X['diff_in_delivery_time'] = X['estimated_delivery_time'] - X['actual_delivery_time']\n",
    "\n",
    "    # finding if delivery was late\n",
    "    X['on_time_delivery'] = X['order_delivered_customer_date'] < X['order_estimated_delivery_date']\n",
    "    X['on_time_delivery'] = X['on_time_delivery'].astype('int')\n",
    "\n",
    "    # calculating mean product value\n",
    "    X['avg_product_value'] = X['price'].astype(float)/X['products_count'].astype(float)\n",
    "\n",
    "    # finding total order cost\n",
    "    X['total_order_cost'] = X['price'].astype(float) + X['freight_value'].astype(float)\n",
    "\n",
    "    # calculating order freight ratio\n",
    "    X['order_freight_ratio'] = X['freight_value'].astype(float)/X['price'].astype(float)\n",
    "\n",
    "    # finding the day of week on which order was made\n",
    "    X['purchase_dayofweek'] = pd.to_datetime(X['order_purchase_timestamp']).dt.dayofweek\n",
    "\n",
    "    # adding is_reviewed where 1 is if review comment is given otherwise 0.\n",
    "    X['is_reviewed'] = (X['review_comment_message'] != 'nan').astype('int')\n",
    "    \n",
    "    drop_columns = ['order_id', 'order_item_id', 'product_id', 'seller_id','shipping_limit_date','customer_id',\n",
    "                    'order_purchase_timestamp', 'order_approved_at', 'order_delivered_customer_date', 'customer_state',\n",
    "                    'order_estimated_delivery_date','customer_unique_id', 'customer_city','customer_zip_code_prefix',\n",
    "                    'order_delivered_carrier_date']\n",
    "    X = X.drop(columns=drop_columns, axis=1)\n",
    "    \n",
    "    num_feat = ['price', 'freight_value', 'product_name_lenght','product_description_lenght', 'product_photos_qty',\n",
    "           'product_weight_g','product_length_cm', 'product_height_cm', 'product_width_cm','sellers_count', \n",
    "           'products_count', 'payment_sequential','payment_installments', 'payment_value','on_time_delivery', \n",
    "           'estimated_delivery_time','actual_delivery_time', 'diff_in_delivery_time','avg_product_value', 'purchase_dayofweek',\n",
    "           'total_order_cost', 'order_freight_ratio','is_reviewed']\n",
    "\n",
    "    # categorical features\n",
    "    cat_feat = ['review_comment_message','product_category_name_english','order_status', 'payment_type']\n",
    "    \n",
    "    # handling missing values \n",
    "    imputer = pickle.load(open('MedianImputer.pkl','rb'))\n",
    "    X[num_feat] = imputer.transform(X[num_feat])\n",
    "    \n",
    "    # processing text and categorical features\n",
    "    X['review_comment_message'] = X['review_comment_message'].fillna('no_review')\n",
    "    X['review_comment_message'] = process_texts(X['review_comment_message'])\n",
    "    X['review_comment_message'] = X['review_comment_message'].replace({'no_review':'nao_reveja'}) \n",
    "    \n",
    "    vectorizer_os = pickle.load(open('vectorizer_order_status.pkl','rb'))\n",
    "    vectorizer_pc = pickle.load(open('vectorizer_product_category.pkl','rb'))\n",
    "    payment_types = pickle.load(open('payment_types.pkl','rb'))\n",
    "    \n",
    "    X['payment_type'] = X['payment_type'].fillna(1)\n",
    "    X['order_status'] = X['order_status'].fillna(vectorizer_os.get_feature_names()[0])\n",
    "    X['product_category_name_english'] = X['product_category_name_english'].fillna(vectorizer_pc.get_feature_names()[0])\n",
    "    \n",
    "    X_os = vectorizer_os.transform(X['order_status'])\n",
    "    X_pc = vectorizer_pc.transform(X['product_category_name_english'])\n",
    "    X['payment_type'] = X['payment_type'].replace(payment_types)\n",
    "    \n",
    "    w2vmodel = Word2Vec.load(\"word2vec.model\")\n",
    "    tfidf_review_comments = pickle.load(open('tfidf_review_comments.pkl','rb'))\n",
    "    # we are converting a dictionary with word as a key, and the idf as a value\n",
    "    tf_values = dict(zip(tfidf_review_comments.get_feature_names(), list(tfidf_review_comments.idf_)))\n",
    "    tfidf_words = set(tfidf_review_comments.get_feature_names())\n",
    "    glove_words = list(w2vmodel.wv.vocab.keys())\n",
    "    \n",
    "    w2v_review_comments = tfidfWord2Vector(X['review_comment_message'].values,glove_words,tfidf_words,tf_values)\n",
    "    \n",
    "    word_index = pickle.load(open('word_index.pkl','rb'))\n",
    "#     embedding_matrix = pickle.load(open('embedding_matrix.pkl','rb'))\n",
    "    encoded_text = []\n",
    "    for y in X['review_comment_message']:\n",
    "        encoded_text.append([word_index[w] if w in word_index else 0 for w in y.split()])\n",
    "    \n",
    "    # pad documents to a max length of 122 words as 95 percentile is 122\n",
    "    max_length = 122\n",
    "    padded_text = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n",
    "    \n",
    "    X = X.drop(labels=['review_comment_message','product_category_name_english','order_status'],axis=1)\n",
    "    \n",
    "    # encoding numerical features\n",
    "    for i in num_feat:\n",
    "        normalizer = pickle.load(open(i+'.pkl','rb'))\n",
    "        X[i] = normalizer.transform(X[i].values.reshape(1,-1))[0]\n",
    "        \n",
    "    # merging our encoded categorical features with rest of the data \n",
    "    X_merge = hstack((X, X_pc, X_os, w2v_review_comments))\n",
    "    X_other = hstack((X, X_pc, X_os))\n",
    "    \n",
    "    return X_merge, X_other, padded_text, X_os, X_pc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(X):\n",
    "    X_merge, X_other, review_padded, X_os, X_pc, x = pre_process(X)\n",
    "    \n",
    "    # reshaping for input to DL models\n",
    "    X_merge_inp = X_merge.toarray().reshape(X_merge.shape[0],X_merge.shape[1],1)\n",
    "    X_other_inp = X_other.toarray().reshape(X_other.shape[0],X_other.shape[1],1)\n",
    "\n",
    "    # ML models\n",
    "#     lr = pickle.load(open('models/logistic.pkl','rb'))\n",
    "#     svm = pickle.load(open('models/svm.pkl','rb'))\n",
    "#     dt = pickle.load(open('models/decision_tree.pkl','rb'))\n",
    "#     rf = pickle.load(open('models/random_forest.pkl','rb'))\n",
    "#     xgb = pickle.load(open('models/xgb.pkl','rb'))\n",
    "#     lgb = pickle.load(open('models/lgbm.pkl','rb'))\n",
    "#     voting = pickle.load(open('models/best_voting.pkl','rb'))\n",
    "#     best_model = pickle.load(open('models/best_stacking.pkl','rb'))\n",
    "    \n",
    "#     # DL models\n",
    "    nn_model = load_model('models/NN_model.h5',custom_objects={'f1':f1}) # input is X_merge\n",
    "    cnn_model1 = load_model('models/cnn_model1.h5',custom_objects={'f1':f1})# input is X_merge_inp\n",
    "    cnn_model2 = load_model('models/cnn_model2.h5',custom_objects={'f1':f1}) # input is review_padded, X_other_inp\n",
    "    lstm_model1 = load_model('models/lstm_model1.h5',custom_objects={'f1':f1}) # input is review_padded, X_os, X_pc, X \n",
    "    lstm_model2 = load_model('models/lstm_model2.h5',custom_objects={'f1':f1}) # input is review_padded, X_other_inp\n",
    "    \n",
    "    # stacked NN models\n",
    "#     nn_soft_stacking = pickle.load(open('models/NN_soft_stacking.pkl','rb'))\n",
    "    nn_hard_stacking = pickle.load(open('models/NN_hard_stacking.pkl','rb'))\n",
    "    \n",
    "    # IF using the NN stacking\n",
    "    y_pred1 = nn_model.predict(X_merge.toarray())\n",
    "    y_pred2 = cnn_model1.predict(X_merge_inp)\n",
    "    y_pred3 = cnn_model2.predict([review_padded, X_other_inp])\n",
    "    y_pred4 = lstm_model1.predict([review_padded, X_os.toarray(), X_pc.toarray(), x])\n",
    "    y_pred5 = lstm_model2.predict([review_padded, X_other_inp])\n",
    "    \n",
    "#     y_pred = nn_soft_stacking.predict(np.stack((y_pred1[:,0],y_pred2[:,0],y_pred3[:,0],y_pred4[:,0],y_pred5[:,0]),axis=-1))\n",
    "    y_pred = nn_hard_stacking.predict(np.stack((np.greater(y_pred1,0.5).astype(int)[:,0],\n",
    "                                                np.greater(y_pred2,0.5).astype(int)[:,0],\n",
    "                                                np.greater(y_pred3,0.5).astype(int)[:,0],\n",
    "                                                np.greater(y_pred4,0.5).astype(int)[:,0],\n",
    "                                                np.greater(y_pred5,0.5).astype(int)[:,0]),axis=-1))\n",
    "# #     y_predicted = lr.predict(X_merge)\n",
    "#     y_predicted = best_model.predict(X_merge)\n",
    "    \n",
    "    return y_pred#y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(X,y):\n",
    "    y = y.apply(lambda x:1 if x>3 else 0)\n",
    "    \n",
    "    y_pred = func1(X)\n",
    "    \n",
    "    print(f1_score(y,y_pred))\n",
    "    \n",
    "    return f1_score(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 5637.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 744 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fcff2a12dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 745 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd0161898c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "CPU times: user 7.84 s, sys: 4.07 s, total: 11.9 s\n",
      "Wall time: 4.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "func1(X_test.iloc[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23663/23663 [00:02<00:00, 10622.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 746 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd015ef23b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.9196365029835223\n",
      "CPU times: user 4min 59s, sys: 5min 4s, total: 10min 4s\n",
      "Wall time: 1min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9196365029835223"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "func2(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23663/23663 [00:02<00:00, 9812.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259279328095367\n",
      "CPU times: user 9.57 s, sys: 564 ms, total: 10.1 s\n",
      "Wall time: 8.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9259279328095367"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "func2(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23663/23663 [00:02<00:00, 9697.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9257074510017126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9257074510017126"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "func2(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
